---
title: "MA615-Assignment4"
author: "Yifan Zhang"
date: "12/5/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
suppressPackageStartupMessages(
  library(tidyverse))
  library(magrittr)
  library(scales)
  library(methods)
  library(knitr)
  library(kableExtra)
  library(janeaustenr)
  library(stringr)
  library(tidytext)
  library(gutenbergr)
  library(scales)
  library(ggplot2)
  library(formattable)
  library(textdata)
  library(tidyr)
  library(wordcloud)
  library(reshape2)
  library(styler)
  library(RColorBrewer)
  source("Book2TN-v6A-1.R")

  opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = TRUE,
  dpi = 300,
  cache.lazy = FALSE,
  tidy = "styler",
  out.width = "90%",
  fig.align = "center",
  fig.width = 5,
  fig.height = 7
)

  options(crayon.enabled = FALSE)
  theme_set(theme_light())
```

# Task 1: Pick a book

The book I picked is _Christmas eve_ by Robert Browning. The poem “Christmas Eve,” by Robert Browning, with the accompanying poem “Easter Day,” seems not to have attracted much notice from the readers of poetry, although highly prized by a few. This is, perhaps, to be attributed, in a great measure, to what many would call a considerable degree of obscurity. 

```{r, include=FALSE}
gutenberg_list <- gutenberg_metadata 
CHRISTMAS <- gutenberg_download(c(6670))
# David <- gutenberg_download(c(766))
# write.table(CHRISTMAS, file = "CHRISTMAS.txt", col.names = FALSE, row.names = FALSE)
# write.table(David, file = "David.txt", col.names = FALSE, row.names = FALSE)
CHRISTMAS <- read.table("CHRISTMAS.txt", header = TRUE)
```

# Task 2: Words analysis

## Words Frequency 

For this part, firstly, I extract `chapters` from txt file and utilize `stop_words` lexicon to remove analytically useless words and calculate rest words' frequencies and proportions. Here is result (only frequency > 10 words will be displayed). 

```{r echo=FALSE, fig.height=3, fig.width=5, fig.cap="Christmas Eve Word Frequency (>10)"}
# convert to one-word-per-row format
tidy_CHRISTMAS <- CHRISTMAS %>% mutate(linenumber = row_number(), 
                 chapter = cumsum(str_detect(text, 
                                             regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) %>% unnest_tokens(word, text)
# remove stop words (not useful for analysis)
tidy_CHRISTMAS <- tidy_CHRISTMAS %>% anti_join(stop_words, by = "word")

Frequency <- tidy_CHRISTMAS %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = reorder(word, n), proportion = formattable::percent(n/sum(n)))

tidy_CHRISTMAS %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = reorder(word, n), proportion = formattable::percent(n/sum(n))) %>% 
  filter(n > 10) %>% 
  ggplot(aes(x=proportion, y=word, fill = word)) + 
  geom_col() + 
  labs(y = NULL, x = "proportion") + 
  theme(legend.position = "none")
```

Figure 1 shows that most common used words in _Christmas Eve_ are `god`, `love`, and `heart`, which account for over 0.6% of total words. To be more clear and get a wider view of it, I also include `wordcloud` to visualize it, which is quite a fancy plot. 

```{r echo=FALSE, fig.height=4.5, fig.width=5, fig.cap="Christmas Eve wordcloud (Top 100 words)"}
# word clouds
tidy_CHRISTMAS %>%
  anti_join(stop_words, by = "word") %>%
  count(word) %>%
  with(wordcloud(word = word, freq = n, max.words = 100, scale=c(3,.2), 
                 random.order = FALSE, rot.per = .35, colors = brewer.pal(6, "Dark2")))
```

Figure 2 shows us Top 100 frequently used words in _Christmas Eve_ and the sizes of words in cloud represent their frequency, the bigger, the more frequent. Actually, only calculate frequencies of a book is not enough. Thus, for the next step, I would go deeper into sentimental analysis of _Christmas Eve_. 


## Sentimental test

As to sentimental analysis, R studio tidyverse provide three sentimental lexicon, `AFINN`, `BING` and `NRC`, for us to do words based analysis. To begin with, I utilize `BING` to label words in _Christmas Eve_ with binary sentimental attitudes, `positive` and `negative`. Here is the visualization of result: 

```{r echo=FALSE, fig.height=2.5, fig.width=5, fig.cap="Christmas Eve Sentiment Analysis (bing)"}
# get_sentiments("afinn")
# get_sentiments("bing")
# get_sentiments("nrc")
# write.table(get_sentiments("bing"), "bing.txt", row.names = FALSE)
# read.table("bing.txt", header = TRUE)
# write.table(get_sentiments("afinn"), "afinn.txt", row.names = FALSE)
# write.table(get_sentiments("nrc"), "nrc.txt", row.names = FALSE)

nrc_joy <- read.table("nrc.txt", header = TRUE) %>% filter(sentiment == "joy")
# tidy_David %>% 
#   inner_join(nrc_joy, by = "word") %>% 
#   count(word, sort = TRUE)

CHRISTMAS_sentiment <- tidy_CHRISTMAS %>%
  inner_join(read.table("bing.txt", header = TRUE), by = "word") %>%
  count(chapter, index = linenumber %/% 20, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

# sentiment by index
ggplot(CHRISTMAS_sentiment, aes(index, sentiment, fill = factor(index))) +
  geom_col(show.legend = FALSE)
```

Figure 3 illustrates that, under the lexicon `BING`, the plot of _Christmas Eve_ changes toward more negative than positive even some of plots indicate fairly positive. Then I decide to utilize more lexicons to continue my sentimental analysis. 

\newpage

```{r echo=FALSE, fig.height=5, fig.width=5, fig.cap="Comparision of  lexicons on Christmas Eve"}
# compare the three sentiment dictionaries
afinn <- tidy_CHRISTMAS %>% 
  inner_join(read.table("afinn.txt", header = TRUE), by = "word") %>% 
  group_by(index = linenumber %/% 20) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
 tidy_CHRISTMAS %>% 
    inner_join(read.table("bing.txt", header = TRUE), by = "word") %>%
    mutate(method = "Bing et al."),
  tidy_CHRISTMAS %>% 
    inner_join(read.table("nrc.txt", header = TRUE) %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    , by = "word") %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 20, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

bind_rows(afinn, select(bing_and_nrc, c(index, sentiment, method))) %>% 
  ggplot(aes(x = index, y = sentiment, fill = method)) + ylim(c(-25, 25)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

The three different lexicons for calculating sentiment give results that are different in an absolute sense but have similar relative trajectories through the novel. We see similar dips and peaks in sentiment at about the same places in the novel, but absolute values are significantly different. 

The AFINN lexicon gives the largest absolute values, with high positive values, while the NRC lexion shows similar patterns but absolute values are relatively smaller. Comparatively, the BING lexicon give a littleb bit more negative result. To figure out why this happens, I decide to look into words in these three lexicons. 

The following table shows proportions of positive and negative words in lexicons: 

```{r include=TRUE, echo=FALSE}
# count how many positive and negative 
rbind(
  read.table("afinn.txt", header = TRUE) %>% mutate(sentiment = ifelse(read.table("afinn.txt", header = TRUE)$value > 0, 
                                                      "positive", "negative")) %>% 
  dplyr::select(word, sentiment) %>% 
  count(sentiment) %>% cbind(lexicon = "afinn") %>% 
  mutate(proportion = formattable::percent(n/sum(n))) %>% 
  dplyr::select(lexicon, sentiment, proportion), 
  
  read.table("nrc.txt", header = TRUE) %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment) %>% cbind(lexicon = "nrc") %>% 
  mutate(proportion = formattable::percent(n/sum(n))) %>% 
  dplyr::select(lexicon, sentiment, proportion), 
  
  read.table("bing.txt", header = TRUE) %>% 
  count(sentiment) %>% cbind(lexicon = "bing") %>% 
  mutate(proportion = formattable::percent(n/sum(n))) %>% 
  dplyr::select(lexicon, sentiment, proportion)
  )
```

\newpage

All three lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the Bing lexicon than the NRC and AFINN lexicon. For the next step, I find out most common positive and nagative words in the book then visualize that in fig 6 and fig 7. 

```{r include=FALSE}
# Most common positive and negative words 
sentiments_pairs <- rbind(
   read.table("afinn.txt", header = TRUE) %>% mutate(sentiment = ifelse(read.table("afinn.txt", header = TRUE)$value > 0, 
                                                      "positive", "negative")) %>% 
  dplyr::select(word, sentiment) %>% cbind(lexicon = "AFINN"), 
  
  read.table("nrc.txt", header = TRUE) %>% 
  filter(sentiment %in% c("positive", "negative")) %>% cbind(lexicon = "NRC"), 
  
  read.table("bing.txt", header = TRUE) %>% cbind(lexicon = "BING")
)
bing_word_count <- tidy_CHRISTMAS %>% 
  inner_join(sentiments_pairs, by = "word") %>% 
  count(word, sentiment, lexicon, sort = TRUE) %>% ungroup()
# visualize top 10 positive and negative words
plot_word_count <- bing_word_count %>%
  group_by(lexicon, sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(lexicon~sentiment, scales = "free_y", ncol = 2) +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

```{r, echo=FALSE, fig.height=7, fig.width=7, fig.cap="Top 10 Negative and Positive Words in Christmas Eve"}
plot_word_count
```

Both of plots indicate that the most common used negative word is `miss` while the most frequently used positive one is `love`. We can use this visualization to see the most important positive and negative words, but the sizes of the words are not comparable across sentiments.


\newpage

# Extra Credits

Apart from `AFINN`, `NRC` and `BING`, I include lexicon `loughran`. here is the proportion of negative and positive words in this lexicon. 

```{r echo=FALSE}
# get_sentiments("loughran") %>% count(sentiment, sort = TRUE)
# write.table(get_sentiments("loughran"), "loughran.txt", row.names = FALSE)
read.table("loughran.txt", header = TRUE) %>% filter(sentiment == "negative" | sentiment == "positive") %>%
  count(sentiment) %>% mutate(proportion =  formattable::percent(n/sum(n)))
```

About this lexicon, it is weird that over 85% of words in this lexicon are `negative`. Next, I utilize it to do the similar research on book _Christmas Eve_. 

```{r echo=FALSE, fig.height=2.5, fig.width=5, fig.cap="Christmas Eve Sentiment Analysis (loughran)"}
CHRISTMAS_sentiment <- tidy_CHRISTMAS %>%
  inner_join(read.table("loughran.txt", header = TRUE)  %>% filter(sentiment == "negative" | sentiment == "positive"),
             by = "word") %>%
  count(chapter, index = linenumber %/% 20, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

# sentiment by chapter
ggplot(CHRISTMAS_sentiment, aes(index, sentiment, fill = factor(index))) +
  geom_col(show.legend = FALSE)
```

No surprise, no matter how plots going on, the sentiment result shows greatly negative emotion, which can be explained by over 85% of words are `negative` in this lexicon. 

\newpage

# Appendix

```{r, echo=FALSE, fig.height=5, fig.width=5, fig.cap="Comparison Cloud (BING)"}
# comparison clouds
tidy_CHRISTMAS %>%
  inner_join(read.table("bing.txt", header = TRUE), by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("dodgerblue3", "firebrick1"),
                   max.words = 100, scale=c(6,.2))
```
